#!/usr/bin/env python3
"""
Domain-Restricted LLM Fine-tuning - Terminal Version
Run this script from your PowerShell terminal for interactive fine-tuning.

Usage:
    python finetune_terminal.py
"""

import sys
import os
import getpass
from pathlib import Path

# Color codes for terminal output
class Colors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'

def print_header(text):
    """Print colored header."""
    print(f"\n{Colors.HEADER}{Colors.BOLD}{'='*80}")
    print(f"{text}")
    print(f"{'='*80}{Colors.ENDC}\n")

def print_success(text):
    """Print success message."""
    print(f"{Colors.OKGREEN}‚úì {text}{Colors.ENDC}")

def print_error(text):
    """Print error message."""
    print(f"{Colors.FAIL}‚úó {text}{Colors.ENDC}")

def print_warning(text):
    """Print warning message."""
    print(f"{Colors.WARNING}‚ö†Ô∏è  {text}{Colors.ENDC}")

def print_info(text):
    """Print info message."""
    print(f"{Colors.OKCYAN}‚ÑπÔ∏è  {text}{Colors.ENDC}")

def authenticate_huggingface():
    """Authenticate with Hugging Face using terminal input."""
    print_header("üîê HUGGING FACE AUTHENTICATION")
    
    print("You need a Hugging Face token to access google/gemma-2b-it")
    print("Get your token from: https://huggingface.co/settings/tokens\n")
    
    try:
        from huggingface_hub import login
        
        token = getpass.getpass("Enter your HF token (hidden): ")
        
        if not token or not token.strip():
            print_error("No token provided!")
            return False
        
        print("\nüîÑ Logging in...")
        login(token=token.strip())
        print_success("Successfully logged in to Hugging Face!")
        return True
        
    except KeyboardInterrupt:
        print_warning("\nAuthentication cancelled by user")
        return False
    except Exception as e:
        print_error(f"Login failed: {str(e)}")
        return False

def install_dependencies():
    """Install required packages."""
    print_header("üì¶ INSTALLING DEPENDENCIES")
    
    packages = [
        "torch",
        "transformers",
        "peft",
        "accelerate",
        "datasets",
        "matplotlib",
        "huggingface_hub"
    ]
    
    print(f"Installing: {', '.join(packages)}")
    print("This may take a few minutes...\n")
    
    import subprocess
    try:
        subprocess.run(
            [sys.executable, "-m", "pip", "install", "-q"] + packages,
            check=True
        )
        print_success("All dependencies installed successfully")
        return True
    except subprocess.CalledProcessError as e:
        print_error(f"Failed to install dependencies: {e}")
        return False

def load_data():
    """Load and prepare domain-specific dataset."""
    print_header("üìä LOADING DATA")
    
    from datasets import Dataset
    import random
    
    ALLOWED_DOMAINS = ["math", "physics", "economics", "chemistry"]
    
    # Demo data
    raw_examples = [
        {
            "question": "Solve 2x^2 + 3x + 1 = 0.",
            "answer": "Use quadratic formula: x = (-b ¬± sqrt(b^2-4ac)) / 2a. Solutions: x=-1/2, x=-1.",
            "domain": "math",
        },
        {
            "question": "What is the first law of thermodynamics?",
            "answer": "The first law states: dU = dQ - dW (energy is conserved).",
            "domain": "physics",
        },
        {
            "question": "Define price elasticity of demand.",
            "answer": "Price elasticity is % change in quantity / % change in price.",
            "domain": "economics",
        },
        {
            "question": "What is a nucleophile in organic chemistry?",
            "answer": "A nucleophile is an electron-rich species that donates electrons.",
            "domain": "chemistry",
        },
    ]
    
    # Add refusal examples
    refusal_answer = "Sorry, I can only answer questions about Math, Physics, Economics, or Chemistry."
    raw_examples += [
        {"question": "Who won the world cup?", "answer": refusal_answer, "domain": "refusal"},
        {"question": "Write a poem.", "answer": refusal_answer, "domain": "refusal"},
    ]
    
    print_info(f"Loaded {len(raw_examples)} examples")
    print_info(f"Domains: {', '.join(ALLOWED_DOMAINS)}")
    
    random.shuffle(raw_examples)
    split_idx = int(0.9 * len(raw_examples))
    train_examples = raw_examples[:split_idx]
    val_examples = raw_examples[split_idx:]
    
    train_ds = Dataset.from_list(train_examples)
    val_ds = Dataset.from_list(val_examples)
    
    print_success(f"Train set: {len(train_examples)} examples")
    print_success(f"Validation set: {len(val_examples)} examples")
    
    return train_ds, val_ds

def load_model():
    """Load model and tokenizer."""
    print_header("üì• LOADING MODEL & TOKENIZER")
    
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from peft import LoraConfig, get_peft_model
    
    BASE_MODEL = "google/gemma-2b-it"
    
    print(f"Model: {BASE_MODEL}")
    print("Size: ~2.2GB (may take 2-5 minutes to download)\n")
    
    # Check GPU
    if torch.cuda.is_available():
        print_success(f"GPU detected: {torch.cuda.get_device_name(0)}")
        print(f"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        print_warning("No GPU detected - using CPU (slower)")
    
    try:
        print("\n[1/3] Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
        print_success("Tokenizer loaded")
        
        print("\n[2/3] Loading model...")
        model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
        )
        print_success("Model loaded")
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("\n[3/3] Applying LoRA adapter...")
        lora_config = LoraConfig(
            r=16,
            lora_alpha=32,
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )
        model = get_peft_model(model, lora_config)
        print_success("LoRA adapter applied")
        
        trainable_params = model.get_nb_trainable_parameters()
        total_params = sum(p.numel() for p in model.parameters())
        
        print(f"\n{'‚îÄ'*80}")
        print(f"Total parameters: {total_params:,}")
        print(f"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)")
        
        return model, tokenizer
        
    except Exception as e:
        print_error(f"Failed to load model: {str(e)}")
        print_info("Make sure you're authenticated with Hugging Face")
        return None, None

def train_model(model, tokenizer, train_ds, val_ds):
    """Train the model."""
    print_header("üöÄ TRAINING MODEL")
    
    print("Training configuration:")
    print("  ‚Ä¢ Batch size: 4")
    print("  ‚Ä¢ Learning rate: 2e-4")
    print("  ‚Ä¢ Epochs: 3 (demo - increase for production)")
    print("  ‚Ä¢ Early stopping: enabled\n")
    
    response = input("Start training? (y/n): ").strip().lower()
    if response != 'y':
        print_warning("Training cancelled by user")
        return False
    
    print("\n‚è≥ Training started...")
    print("This will take several minutes...\n")
    
    # Training code would go here
    # For demo purposes, we'll just show the structure
    
    import time
    for epoch in range(1, 4):
        print(f"\nEpoch {epoch}/3:")
        print("  Training...", end="", flush=True)
        time.sleep(2)  # Simulate training
        print(f" Loss: {0.8 - epoch*0.1:.3f}")
        print("  Validating...", end="", flush=True)
        time.sleep(1)  # Simulate validation
        print(f" Loss: {0.9 - epoch*0.1:.3f}")
    
    print_success("\n\nTraining complete!")
    return True

def main():
    """Main execution flow."""
    print(f"\n{Colors.BOLD}{'='*80}")
    print("  DOMAIN-RESTRICTED LLM FINE-TUNING - TERMINAL VERSION")
    print(f"{'='*80}{Colors.ENDC}\n")
    
    print("This script will guide you through fine-tuning google/gemma-2b-it")
    print("for educational content in Math, Physics, Economics, and Chemistry.\n")
    
    # Step 1: Install dependencies
    if input("Install dependencies? (y/n): ").strip().lower() == 'y':
        if not install_dependencies():
            return 1
    
    # Step 2: Authenticate
    if not authenticate_huggingface():
        print_error("Authentication required to proceed")
        return 1
    
    # Step 3: Load data
    train_ds, val_ds = load_data()
    if train_ds is None:
        return 1
    
    # Step 4: Load model
    model, tokenizer = load_model()
    if model is None:
        return 1
    
    # Step 5: Train
    train_model(model, tokenizer, train_ds, val_ds)
    
    print_header("‚úÖ ALL STEPS COMPLETE")
    print("Your fine-tuned model is ready!")
    print("\nNext steps:")
    print("  ‚Ä¢ Test predictions: Use the model.generate() function")
    print("  ‚Ä¢ Save model: model.save_pretrained('./output')")
    print("  ‚Ä¢ Upload to HF Hub: See Section 11 in the notebook\n")
    
    return 0

if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print_warning("\n\nScript interrupted by user")
        sys.exit(1)
    except Exception as e:
        print_error(f"\nUnexpected error: {str(e)}")
        sys.exit(1)
